@article{Zhou2015,
abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1{\%} top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2{\%} top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them},
archivePrefix = {arXiv},
arxivId = {1512.04150},
author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
doi = {10.1109/CVPR.2016.319},
eprint = {1512.04150},
file = {:C$\backslash$:/Users/jmiller/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2015 - Learning Deep Features for Discriminative Localization.pdf:pdf},
journal = {arXiv:1512.04150 [cs]},
mendeley-groups = {Paper},
title = {{Learning Deep Features for Discriminative Localization}},
url = {http://arxiv.org/abs/1512.04150{\%}5Cnhttp://www.arxiv.org/pdf/1512.04150.pdf},
year = {2015}
}
@article{Lin2014,
abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
archivePrefix = {arXiv},
arxivId = {1312.4400},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
doi = {10.1109/ASRU.2015.7404828},
eprint = {1312.4400},
file = {:C$\backslash$:/Users/jmiller/Dropbox/articles/1312.4400v3.pdf:pdf},
isbn = {9781479972913},
issn = {03029743},
journal = {arXiv preprint},
pages = {10},
pmid = {24356345},
title = {{Network In Network}},
url = {http://arxiv.org/abs/1312.4400},
year = {2014}
}
